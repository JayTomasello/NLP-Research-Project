"""
pushshiftDFSetupTC.py

This program modifies the dataframe generated by pushshift.py for the purposes 
of performing a significant term count.

Author: Joseph A. Tomasello 
"""


"""
BEGIN - SCRIPT PREPARATION
"""
import pandas as pd
import re
import spacy
nlp = spacy.load('en_core_web_trf')
import string
string.punctuation
import nltk
wn = nltk.WordNetLemmatizer()
ps = nltk.PorterStemmer()
stopword = nltk.corpus.stopwords.words('english')
import nltk
nltk.data.path.append("/path/to/nltk_data")
"""
END - SCRIPT PREPARATION 
"""


"""
BEGIN - FUNCTIONS
"""
# This function uses the csv dataset file passed to it to create a dataframe and prep
# the selftext column (the post's body text) for analysis use
def create_dataframe(dataset_filename, analysis_column):

    # Creates a dataframe using a dataset file
    dataframe = pd.read_csv(dataset_filename, encoding="utf8")
    
    # Filters out any collected posts that were removed by the subreddit's moderators
    dataframe = dataframe[dataframe.selftext != "[removed]"]
    # Filters out any collected posts that were deleted by the post's author
    dataframe = dataframe[dataframe.selftext != "[deleted]"]
    
    # Adds a new column to the dataframe to store a cleaned version (stripped of special 
    # characters and non-relevant text) of the selftext for each collected post
    dataframe['clean_body_text'] = dataframe[analysis_column].apply( lambda x: clean_text_TA( str(x) ) )

    return dataframe

# This helper function removes all characters and text not relevant to performing a term 
# count (i.e., special characters, punctuation, and URLs)
def clean_text_TA(text):
    
    text = text.replace('\n', ' ')            # Removes any newline characters
    text = text.replace('\r\r', ' ')          # Removes any carriage return characters
    text = text.replace('&amp;#x200B;', ' ')  # Removes any zero-width space characters
    text = re.sub(r'http\S+', '', text)       # Removes any URLs beginning with http
    text = re.sub(r'www\S+', '', text)        # Removes any URLs beginning with www
    text = text.replace('_', ' ')             # Removes any _ characters
    text = text.replace('--', ' ')            # Removes any -- characters
    text = text.replace('-', ' ')             # Removes any - characters
    text = text.replace(',', ' ')             # Removes any , characters (Example: Also,I would... --> Also I would...)
    text = text.replace('"', ' ')             # Removes any " characters (Example: An "easy"task --> An easy task)
    text = text.replace('/', ' ')             # Removes any / characters (Example: small/medium/large --> small medium large)
    text = text.replace('(', ' ')             # Removes any ( characters (Example: His(Kevin's)car --> His Kevin's)car)
    text = text.replace(')', ' ')             # Removes any ) characters (Example: His Kevin's)car --> His Kevin's car)
    text = text.replace('?', ' ')             # Removes any ? characters (Example: What?Who said that? --> What Who said that)
    text = text.replace('!', ' ')             # Removes any ! characters (Example: Oh no!What will... --> Oh no What will...)
    text = re.sub(r'\.(?!\d)', ' ', text)     # Removes any . characters unless they are part of a number (Example: Yes.No problem. --> Yes No problem)
    text_clean = re.sub(r'[^\w\s]', '', text) # Removes any remaining punctuation
    
    return text_clean

# This function appends columns to the dataframe useful for performing a term count
def term_count(dataframe):

    dataframe['tokenized_body_text'] = dataframe['clean_body_text'].apply( lambda x: tokenize( str(x).lower() ) )
    dataframe['body_text_no_stop'] = dataframe['tokenized_body_text'].apply( lambda x: remove_stopwords(x) )
    dataframe['body_text_stemmed'] = dataframe['body_text_no_stop'].apply( lambda x: stemming(x) )
    dataframe['body_text_lemmatized'] = dataframe['body_text_no_stop'].apply( lambda x: lemmatizing(x) )

    return dataframe

# This helper function tokenizes the dataframe's clean_body_text data
def tokenize(text):
    tokens = re.split('\W+', str(text))
    return tokens

# This helper function removes stopwords from the tokenized text data
def remove_stopwords(tokenized_list):
    text = [word for word in tokenized_list if word not in stopword]
    return text

# This helper function stems the tokenized text data removed of stopwords
def stemming(body_text_no_stop):
    text = [ps.stem(word) for word in body_text_no_stop]
    return text

# This helper function lemmatizes the tokenized text data removed of stopwords
def lemmatizing(body_text_no_stop):
    text = [wn.lemmatize(word) for word in body_text_no_stop]
    return text

# This function filters posts out of the dataframe collected from a particular subreddit
def subreddit_filter(dataframe, subreddit_column, subreddit_name):

    # Filters out any collected posts from the subreddit specifed by the user
    filtered_dataframe = dataframe[ dataframe[subreddit_column] != subreddit_name ]

    return filtered_dataframe

# This function converts and saves the dataframe passed to it as a csv file
def save_dataframe_as_csv(dataframe, file_name):

    # Converts and saves the dataframe to a csv file
    dataframe.to_csv(file_name, index=False)
"""
END - FUNCTIONS
"""
